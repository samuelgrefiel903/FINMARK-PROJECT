{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3d8127a",
   "metadata": {},
   "source": [
    "# Customer Purchase Behavior — EDA, Cleaning, and Feature Engineering\n",
    "\n",
    "This notebook is designed to run **top-to-bottom** with minimal edits.\n",
    "\n",
    "## What it does\n",
    "1. Loads the dataset reliably (handles common CSV issues).\n",
    "2. Validates structure (shape, columns, dtypes, basic stats).\n",
    "3. EDA focused on **customer purchase prediction**:\n",
    "   - Missing values (table + bar chart)\n",
    "   - Data types / mixed types\n",
    "   - Duplicates\n",
    "   - Outliers **only for customer-relevant numeric columns**\n",
    "4. Preprocessing:\n",
    "   - Fixes missing tokens and trims strings\n",
    "   - Parses date/time columns when applicable\n",
    "   - Drops constant and ID-like columns\n",
    "   - Flags and (optionally) drops likely **leakage/irrelevant** columns (e.g., company profit)\n",
    "   - Imputes missing values (median/mode)\n",
    "   - Caps outliers (IQR)\n",
    "5. Feature engineering:\n",
    "   - Datetime features\n",
    "   - Frequency encodings for categoricals\n",
    "   - Log transforms for skewed numeric columns\n",
    "6. Saves:\n",
    "   - `customers_cleaned.csv`\n",
    "   - `customers_features.csv` (ML-ready, one-hot encoded)\n",
    "\n",
    "> Tip for team discussion: This notebook also prints a **Decision Log** so you can explain what changed and why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ebcb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) Imports & Settings\n",
    "# ============================================================\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
    "\n",
    "# ---- Set your CSV path here ----\n",
    "# (A) If running in this sandbox environment, the default below will work.\n",
    "# (B) If running locally on Windows, set something like:\n",
    "# DATA_PATH = r\"C:\\Users\\lawre\\Downloads\\customers_data (1).csv\"\n",
    "DATA_PATH = r\"/mnt/data/customers_data (1).csv\"\n",
    "\n",
    "OUT_DIR = r\"./eda_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "CLEAN_PATH    = os.path.join(OUT_DIR, \"customers_cleaned.csv\")\n",
    "FEATURES_PATH = os.path.join(OUT_DIR, \"customers_features.csv\")\n",
    "REPORT_PATH   = os.path.join(OUT_DIR, \"eda_decision_log.txt\")\n",
    "\n",
    "# Optional: if you KNOW your target column name, set it.\n",
    "# Example: TARGET_COL = \"Purchased\"\n",
    "TARGET_COL = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25beb12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) Robust CSV Loader (fixes most read errors)\n",
    "# ============================================================\n",
    "def robust_read_csv(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"File not found: {path}\\n\"\n",
    "            \"Fix: update DATA_PATH to your real file location.\"\n",
    "        )\n",
    "    attempts = [\n",
    "        {\"encoding\": \"utf-8\", \"sep\": \",\"},\n",
    "        {\"encoding\": \"utf-8-sig\", \"sep\": \",\"},\n",
    "        {\"encoding\": \"cp1252\", \"sep\": \",\"},\n",
    "        {\"encoding\": \"latin1\", \"sep\": \",\"},\n",
    "        {\"encoding\": \"utf-8\", \"sep\": \";\"},\n",
    "        {\"encoding\": \"cp1252\", \"sep\": \";\"},\n",
    "    ]\n",
    "    last_err = None\n",
    "    for opts in attempts:\n",
    "        try:\n",
    "            df = pd.read_csv(path, **opts)\n",
    "            # If it became 1 column, likely wrong separator; keep trying\n",
    "            if df.shape[1] == 1 and opts[\"sep\"] == \",\":\n",
    "                continue\n",
    "            print(f\"✅ Loaded with encoding={opts['encoding']} sep='{opts['sep']}' | shape={df.shape}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Could not read CSV. Last error: {last_err}\")\n",
    "\n",
    "df_raw = robust_read_csv(DATA_PATH)\n",
    "df_raw.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ef999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) Validate Structure\n",
    "# ============================================================\n",
    "print(\"Shape:\", df_raw.shape)\n",
    "print(\"\\nColumns:\")\n",
    "print(list(df_raw.columns))\n",
    "\n",
    "print(\"\\nInfo:\")\n",
    "df_raw.info()\n",
    "\n",
    "print(\"\\nBasic numeric stats:\")\n",
    "display(df_raw.describe(include=[np.number]).T.head(20))\n",
    "\n",
    "print(\"\\nBasic categorical stats (top columns):\")\n",
    "cat_cols = df_raw.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "display(df_raw[cat_cols].describe().T.head(20) if len(cat_cols) else \"No object columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dce764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3) EDA (Focused for Purchase Prediction)\n",
    "# ============================================================\n",
    "\n",
    "# 3.1 Missing values\n",
    "missing = df_raw.isna().sum().sort_values(ascending=False)\n",
    "missing_nonzero = missing[missing > 0]\n",
    "display(missing_nonzero if len(missing_nonzero) else \"✅ No missing values detected.\")\n",
    "\n",
    "# Bar plot of missing values\n",
    "if len(missing_nonzero):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    missing_nonzero.head(30).plot(kind=\"bar\")\n",
    "    plt.title(\"Missing Values per Column (Top 30)\")\n",
    "    plt.ylabel(\"Count missing\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 3.2 Duplicates\n",
    "dup_count = df_raw.duplicated().sum()\n",
    "print(\"Duplicate rows:\", dup_count)\n",
    "\n",
    "# 3.3 Mixed types detection (object columns usually)\n",
    "mixed = []\n",
    "for c in df_raw.columns:\n",
    "    # Check within non-null values to avoid NaN type noise\n",
    "    types = df_raw[c].dropna().map(type).nunique()\n",
    "    if types > 1:\n",
    "        mixed.append(c)\n",
    "print(\"Columns with mixed Python types:\", mixed if mixed else \"None\")\n",
    "\n",
    "# 3.4 Candidate 'irrelevant/leakage' columns (heuristics)\n",
    "leakage_keywords = [\"profit\", \"revenue\", \"margin\", \"net_income\", \"ebit\", \"earnings\", \"company\"]\n",
    "leakage_cols = [c for c in df_raw.columns if any(k in str(c).lower() for k in leakage_keywords)]\n",
    "print(\"⚠️ Potential leakage/irrelevant columns (review):\", leakage_cols if leakage_cols else \"None\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02100d",
   "metadata": {},
   "source": [
    "## Outliers: Why you saw “Company Profit”\n",
    "\n",
    "Your earlier code plotted *every numeric column*, so if a column like `Company_Profit` is numeric it gets included automatically.\n",
    "\n",
    "For **customer purchase prediction**, company-level profit often behaves like:\n",
    "- **not customer behavior**, and/or\n",
    "- **data leakage** (information you wouldn't know at the time of purchase)\n",
    "\n",
    "So below we:\n",
    "1) Identify **customer-relevant numeric columns** (exclude obvious company/profit/revenue),\n",
    "2) Plot outliers only for those.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Outlier check (only customer-relevant numeric columns)\n",
    "num_cols = df_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Exclude likely leakage/company finance columns from outlier charts\n",
    "exclude_outlier_keywords = [\"profit\", \"revenue\", \"margin\", \"net_income\", \"earnings\", \"ebit\", \"company\"]\n",
    "num_cols_focus = [c for c in num_cols if not any(k in str(c).lower() for k in exclude_outlier_keywords)]\n",
    "\n",
    "print(\"Numeric columns (all):\", num_cols)\n",
    "print(\"Numeric columns (outlier plots focus):\", num_cols_focus)\n",
    "\n",
    "# Boxplots for up to 12 focused numeric columns\n",
    "for c in num_cols_focus[:12]:\n",
    "    plt.figure()\n",
    "    sns.boxplot(x=df_raw[c])\n",
    "    plt.title(f\"Outlier Check (Focused): {c}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4) Preprocessing (with Decision Log)\n",
    "# ============================================================\n",
    "DECISIONS = []\n",
    "\n",
    "def log(msg: str):\n",
    "    DECISIONS.append(msg)\n",
    "    print(\"•\", msg)\n",
    "\n",
    "def normalize_missing_strings(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str).str.strip()\n",
    "    s = s.replace({\n",
    "        \"\": np.nan, \" \": np.nan,\n",
    "        \"nan\": np.nan, \"NaN\": np.nan,\n",
    "        \"none\": np.nan, \"None\": np.nan,\n",
    "        \"null\": np.nan, \"NULL\": np.nan,\n",
    "        \"na\": np.nan, \"N/A\": np.nan, \"n/a\": np.nan\n",
    "    })\n",
    "    return s\n",
    "\n",
    "def try_parse_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\":\n",
    "            name = str(c).lower()\n",
    "            if any(k in name for k in [\"date\", \"time\", \"dt\", \"timestamp\"]):\n",
    "                parsed = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n",
    "                if parsed.notna().mean() >= 0.5:\n",
    "                    df[c] = parsed\n",
    "                    log(f\"Parsed datetime column: {c}\")\n",
    "    return df\n",
    "\n",
    "def drop_constant_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    nunique = df.nunique(dropna=False)\n",
    "    constant_cols = nunique[nunique <= 1].index.tolist()\n",
    "    if constant_cols:\n",
    "        df = df.drop(columns=constant_cols)\n",
    "        log(f\"Dropped constant columns: {constant_cols}\")\n",
    "    return df\n",
    "\n",
    "def drop_id_like_columns(df: pd.DataFrame, threshold_ratio: float = 0.98) -> pd.DataFrame:\n",
    "    n = len(df)\n",
    "    drop_cols = []\n",
    "    for c in df.columns:\n",
    "        if TARGET_COL is not None and c == TARGET_COL:\n",
    "            continue\n",
    "        unique_ratio = df[c].nunique(dropna=False) / max(n, 1)\n",
    "        cname = str(c).lower()\n",
    "        if unique_ratio >= threshold_ratio and any(k in cname for k in [\"id\", \"uid\", \"key\", \"code\", \"number\", \"no\"]):\n",
    "            drop_cols.append(c)\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "        log(f\"Dropped ID-like columns: {drop_cols}\")\n",
    "    return df\n",
    "\n",
    "def drop_leakage_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Heuristic only—review the printed list if needed\n",
    "    leakage_keywords = [\"profit\", \"revenue\", \"margin\", \"net_income\", \"earnings\", \"ebit\", \"company\"]\n",
    "    candidates = [c for c in df.columns if any(k in str(c).lower() for k in leakage_keywords)]\n",
    "    if candidates:\n",
    "        # Drop them by default because this notebook is for customer purchase behavior.\n",
    "        df = df.drop(columns=candidates)\n",
    "        log(f\"Dropped likely leakage/irrelevant company-finance columns: {candidates}\")\n",
    "    return df\n",
    "\n",
    "def impute_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].median())\n",
    "            log(f\"Imputed numeric missing with median: {c}\")\n",
    "\n",
    "    for c in cat_cols:\n",
    "        if df[c].isna().any():\n",
    "            mode = df[c].mode(dropna=True)\n",
    "            fill_val = mode.iloc[0] if len(mode) else \"Unknown\"\n",
    "            df[c] = df[c].fillna(fill_val)\n",
    "            log(f\"Imputed categorical missing with mode: {c}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def cap_outliers_iqr(df: pd.DataFrame, factor: float = 1.5) -> pd.DataFrame:\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for c in num_cols:\n",
    "        x = df[c].dropna()\n",
    "        if x.empty:\n",
    "            continue\n",
    "        q1, q3 = x.quantile(0.25), x.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        if iqr == 0:\n",
    "            continue\n",
    "        lo, hi = q1 - factor * iqr, q3 + factor * iqr\n",
    "        df[c] = df[c].clip(lo, hi)\n",
    "    log(\"Capped numeric outliers using IQR clipping (kept all rows).\")\n",
    "    return df\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize strings + missing tokens\n",
    "    for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        df[c] = normalize_missing_strings(df[c])\n",
    "    log(\"Trimmed strings and normalized common missing tokens in object columns.\")\n",
    "\n",
    "    df = try_parse_dates(df)\n",
    "\n",
    "    # Duplicates\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    removed = before - len(df)\n",
    "    if removed:\n",
    "        log(f\"Removed duplicate rows: {removed}\")\n",
    "    else:\n",
    "        log(\"No duplicate rows removed.\")\n",
    "\n",
    "    df = drop_constant_columns(df)\n",
    "    df = drop_id_like_columns(df)\n",
    "    df = drop_leakage_columns(df)\n",
    "\n",
    "    df = impute_missing(df)\n",
    "    df = cap_outliers_iqr(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_clean = preprocess(df_raw)\n",
    "\n",
    "print(\"\\n✅ Cleaned shape:\", df_clean.shape)\n",
    "df_clean.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick re-check after cleaning\n",
    "print(\"Missing after cleaning:\", int(df_clean.isna().sum().sum()))\n",
    "df_clean.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23398a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5) Feature Engineering\n",
    "# ============================================================\n",
    "def feature_engineering(df: pd.DataFrame, target_col: str = None) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # Datetime features\n",
    "    dt_cols = out.select_dtypes(include=[\"datetime64[ns]\"]).columns.tolist()\n",
    "    for c in dt_cols:\n",
    "        out[f\"{c}_year\"] = out[c].dt.year\n",
    "        out[f\"{c}_month\"] = out[c].dt.month\n",
    "        out[f\"{c}_day\"] = out[c].dt.day\n",
    "        out[f\"{c}_dayofweek\"] = out[c].dt.dayofweek\n",
    "        out[f\"{c}_is_weekend\"] = out[c].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    if dt_cols:\n",
    "        log(f\"Created datetime features from: {dt_cols}\")\n",
    "\n",
    "    # Numeric log transforms (helps skew)\n",
    "    num_cols = out.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col in num_cols:\n",
    "        num_cols.remove(target_col)\n",
    "    for c in num_cols:\n",
    "        if out[c].min() >= 0:\n",
    "            out[f\"log1p_{c}\"] = np.log1p(out[c])\n",
    "    if num_cols:\n",
    "        log(\"Created log1p features for non-negative numeric columns.\")\n",
    "\n",
    "    # Frequency encoding for categoricals (strong baseline feature)\n",
    "    cat_cols = out.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    if target_col in cat_cols:\n",
    "        cat_cols.remove(target_col)\n",
    "    for c in cat_cols:\n",
    "        freq = out[c].value_counts(dropna=False) / len(out)\n",
    "        out[f\"{c}_freq\"] = out[c].map(freq)\n",
    "    if cat_cols:\n",
    "        log(\"Created frequency-encoding features for categorical columns.\")\n",
    "\n",
    "    return out\n",
    "\n",
    "df_feat = feature_engineering(df_clean, target_col=TARGET_COL)\n",
    "print(\"✅ Features shape:\", df_feat.shape)\n",
    "df_feat.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ae6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6) ML-ready Encoding (One-Hot)\n",
    "# ============================================================\n",
    "def make_ml_ready(df: pd.DataFrame, target_col: str = None) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    y = None\n",
    "    if target_col is not None and target_col in out.columns:\n",
    "        y = out[target_col]\n",
    "        out = out.drop(columns=[target_col])\n",
    "\n",
    "    cat_cols = out.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    out_enc = pd.get_dummies(out, columns=cat_cols, drop_first=True)\n",
    "\n",
    "    if y is not None:\n",
    "        out_enc[target_col] = y.values\n",
    "\n",
    "    return out_enc\n",
    "\n",
    "df_ml = make_ml_ready(df_feat, target_col=TARGET_COL)\n",
    "print(\"✅ ML-ready shape:\", df_ml.shape)\n",
    "df_ml.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7) Save Outputs + Decision Log\n",
    "# ============================================================\n",
    "df_clean.to_csv(CLEAN_PATH, index=False)\n",
    "df_ml.to_csv(FEATURES_PATH, index=False)\n",
    "\n",
    "with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(DECISIONS))\n",
    "\n",
    "print(\"✅ Saved:\")\n",
    "print(\" - Cleaned CSV   :\", CLEAN_PATH)\n",
    "print(\" - Features CSV  :\", FEATURES_PATH)\n",
    "print(\" - Decision Log  :\", REPORT_PATH)\n",
    "\n",
    "print(\"\\n--- Decision Log Preview ---\")\n",
    "for line in DECISIONS[:20]:\n",
    "    print(\"•\", line)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}